@node Available tools
@chapter Available tools

The main goal of Marsyas is to provide an extensible framework that
can be used to quickly design and experiment with audio analysis and
synthesis applications.  The tools provided with the distribution,
although useful, are only representative examples of what can be
achieved using the provided components.  Marsyas is an extensible
framework for building applications, so the primary purpose of these
examples is to provide source code of working applications.

The executable files may be found in the @file{bin/} subdirectory
of the build directory, while the source code for those files is
in @file{src/apps/@{DIR@}/}.

@WANTED{descriptions of all these programs}

@menu
* Collections and input files::  
* Soundfile Interaction::       
* Feature Extraction::          
* Synthesis::                   
* Machine Learning::            
* Auditory Scene Analysis::     
* Marsystem Interaction::       
* All of the above::            
@end menu


@node Collections and input files
@section Collections and input files

Many Marsyas tools can operate on individual soundfiles or
collections of soundfiles.  A collection is a simple text
file which contain lists of soundfiles. 

@menu
* Creating collections manually::  
* mkcollection::                
@end menu


@node Creating collections manually
@subsection Creating collections manually

A simple way to create a collection is the unix ls command. 
For example: 

@example
ls /home/gtzan/data/sound/reggae/*.wav > reggae.mf
@end example

@noindent
@code{reggae.mf} will look like this:

@example
/home/gtzan/data/sound/reggae/foo.wav 
/home/gtzan/data/sound/reggae/bar.wav 
@end example

Any text editor can be used to create collection files. The only
constraint is that the name of the
collections file must have a @code{.mf} extension such as
@code{reggae.mf}.   In addition, any
line starting with the @code{#} character is ignored.  For Windows Visual
Studio, change the slash character separating directories appropriately.


@subsection Labels

Labels may be added to collections by appending tab-seperated labels
after each sound file:

@example
/home/gtzan/data/sound/reggae/foo.wav \t music
/home/gtzan/data/sound/reggae/bar.wav \t speech
@end example

The \t represents an actual tab character.  This allows you to create a
@qq{master} collection which includes different kinds of labelled sound
files:

@example
cat music.mf speech.mf > all.mf
@end example


@subsection @code{MARSYAS_DATADIR}

Collections support the environment variable
@code{MARSYAS_DATADIR}.  This allows the use of @code{.mf} files
shared between users (i.e. for a large dataset of audio).  For
example, the above collection could be rewritten as:

@example
MARSYAS_DATADIR/reggae/foo.wav \t music
MARSYAS_DATADIR/reggae/bar.wav \t speech
@end example

@noindent
provided that the user configures the environment variable
appropriately.  For example, using bash on Linux or MacOS X, users
on three different machines may set up the variable as:

@example
export MARSYAS_DATADIR=/home/gtzan/data/sound/
export MARSYAS_DATADIR=/Users/gtzan/data/sound/
export MARSYAS_DATADIR=/home/gperciva/media/marsyas-data/
@end example


@node mkcollection
@subsection @code{mkcollection}
@cindex mkcollection     
 
@code{mkcollection} is a simple utility for creating collection 
files. To create a collection of all the audio files residing 
in a directory the following command can be used: 

@example 
mkcollection -c reggae.mf -l music /home/gtzan/data/sound/
@end example

This also labels the data as @samp{music}.

If the @code{-md} flag is added, then the filenames written to the
@code{.mf} file will contain @code{MARSYAS_DATADIR} when
appropriate.

All the soundfiles residing in that directory or any subdirectories
will be added to the collection.  @code{mkcollection} only will add
files with @code{.wav} and @code{ .au} extensions but
does not check that they are valid soundfiles.  In general
collection files should contain soundfiles with the same sampling rate
as Marsyas does not perform automatic sampling conversion on collections. 

@warning{@code{mkcollection} will add a @file{.mf} to the
collection filename if it does not contain any extention;
otherwise it will leave the collection filename alone.}


@node Soundfile Interaction, Feature Extraction, Collections and input files, Available tools
@section Soundfile Interaction

@menu
* sfplay::                      
* sfinfo::                      
* audioCompare::                
* record::                      
* orcarecord::                  
* sound2png::                  
* sound2sound::                  
@end menu


@node sfplay, sfinfo, Soundfile Interaction, Soundfile Interaction
@subsection @code{sfplay}
@cindex sfplay 

@code{sfplay} is a flexible command-line soundfile player that allows
playback of multiple soundfiles in various formats with either
real-time audio output or soundfile output. The following two example
show two extremes of using of sfplay: simple playback of foo.wav and
playing 3.2 seconds (-l) clips starting at 10.0 seconds (-s) into the
file and repeating the clips for 2.5 times (-r) writing the output to
output.wav (-f) at half volume (-g) playing each file in the
collection reggae.mf. Using the (--ws) option, you may manually set
the window size in samples. The last command stores the MarSystem
dataflow network used in sfplay as a plugin in playback.mpl. The
plugin is essentially a textual description of the created network.
Because MarSystems can be created at run-time the network can be
loaded in a sfplugin which is a generic executable that flows audio
data through any particular network.  Running sfplugin -p playback.mpl
bar.wav will play using the created plugin the file bar.wav. It is
important to note that although both sfplay and sfplugin have the same
behavior in this case they achieve it very different.  The main
difference is that in sfplay the network is created at compile time
wheras in sfplugin the network is created at run time.


@example
sfplay foo.wav 
sfplay -s 10.0 -l 3.2 -r 2.5 -g 0.5 foo.wav bar.au -f output.wav
sfplay -l 3.0 reggae.mf
sfplay foo.wav -p playback.mpl 
sfplugin -p playback.mpl bar.wav
@end example


@node sfinfo, audioCompare, sfplay, Soundfile Interaction
@subsection @code{sfinfo}

sfinfo displays information (number of channels, sampling rate,
durations) about a support soundfile (.wav and .au by default 
.mp3 .ogg if the appropriate extensions are installed) 

@example 
sfinfo foo.wav 
@end example 


@node audioCompare, record, sfinfo, Soundfile Interaction
@subsection @code{audioCompare}

Compares two audio files for similarity with some small tolerance 
for sample differences. It returns 0 if the files are the same and 1 
if they are they are not. It is used for internal testing. 



@node record, orcarecord, audioCompare, Soundfile Interaction
@subsection @code{record}

Records to a sound-file using the default audio input device.  For
example the second command records 5 seconds of stereo (-c 2) audio at
a sampling rate of 22050 at 80 percent volume (-g 0.8). The results
are written to foo.wav.


@example 
record -l 3 foo.wav 
record -l 5 -c 2 -s 22050 -g 0.8 bar.wav 
@end example 




@node orcarecord, sound2png , record, Soundfile Interaction
@subsection @code{orcarecord}

Similar to record but with specic change to record 4 stereo channels
in parallel using a TASCAM audio interface. Created for the
digitization of tapes for the Orchive project. As in the previous
example, this following commmand records 8 seconds (-l 8) of audio
input at 30 percent volume (-g 0.3).


@example 
orcarecord -l 8 -g 0.3 foo.wav 
@end example 


@node sound2png, sound2sound , orcarecord, Soundfile Interaction
@subsection @code{sound2png}

A program that uses Marsyas to generate a PNG of an input audio file.
The PNG can be either the waveform or the spectrogram of the audio
file.

When generating a spectrogram, you can set both the window size and
hop size that are used in calculating the FFT.  The window size that
you give is then used as the amount of data that the FFT is given,
which means that the number of bins for the FFT will be half of the
window size.  Each bin of the FFT will be drawn in one pixel
vertically, so if you use a window size of 512, the resulting PNG will
be 256 pixels high.

The hop size for the spectrogram tells the program how much to overlap
each FFT by.  The width of the output PNG will thus depend on the
length of the audio file and the hop size, with smaller hop sizes
giving longer PNG images.

Below is shown an example of using sound2png to generate a spectrogram
of an orca call.  We use a window size of 1024 and a hop size of 1024.
The maximum frequency is set to 8000Hz.  A gain of 1.5 is used to make
the spectrogram darker:

@example 
sound2png -m spectrogram A30.wav -ws 1024 -hs 1024 -mf 8000 -g 1.5 out.png
@end example 

@image{images/sound2png_spectrogram,,5cm}

You can also you sound2png to generate pictures of the waveform of an
audio file.  For this, you use the -w option.  An example of this is
shown below:

@example 
sound2png -m waveform tiny.wav -ws 1 out.png
@end example 

@image{images/sound2png_waveform,,5cm}

When generating pictures of waveforms, you can specify a window size.
sound2png takes a chunk of data that is window size samples in length
and calculates the maximum and minimum of this window.  It then draws
a bar from the minimum to the maximum value for each window.  An
example of this is shown below:

@example 
sound2png -m waveform small.wav -ws 100 out.png
@end example 

@image{images/sound2png_waveform2,,5cm}

@node sound2sound,  , sound2png, Soundfile Interaction
@subsection @code{sound2sound}

A program that uses Marsyas to do various types of audio                                                                    
processing/digital audio effects that takes as input a single                                                               
audio file and generate a single audio file that is the result of                                                           
the processing.   

Help info (this should give you all the parameters that you can adjust 
from the interface):

@example
sound2sound -h
@end example


Bandpass filter with center frequency 500 Hz and a q-factor of 10 the
file input.wav processing 30 seconds starting 15 seconds into the file
with a gain of 0.8 and write the result to output.wav

@example
sound2sound input.wav -m bandpass -f 500 -q 10 -s 15 -l 30 -g 0.8 output.wav
@end example

Equivalently:
@example
sound2sound input.wav --method bandpass --frequency 500 --qfactor 10 --start 15 --length 30 --gain 0.8 output.wav
@end example


@node Feature Extraction, Synthesis, Soundfile Interaction, Available tools
@section Feature Extraction

@menu
* pitchextract::                
* bextract::
* ibt::                     
@end menu


@node pitchextract, bextract, Feature Extraction, Feature Extraction
@subsection @code{pitchextract}
@cindex pitchextract

@code{pitchextract} is used to extract the fundamental frequency
contour from monophonic audio signals. A simple sinusoidal playback is
provided for playback of the resulting contour. In the following
example, the pich is extracted from soundfile, using a windowsize of
1024 (-w 1024), a lower pitch of 36 (-l 36) and an upper pitch of 128
(-u 128).


@example 
pitchextract -w 1024 -l 36 -u 128 soundfile
@end example 



@node bextract, ibt, pitchextract, Feature Extraction
@subsection @code{bextract}
@cindex bextract

@code{bextract} @cindex bextract 
is one of the most powerful
executables provided by Marsyas. It can be used for complete feature
extraction and classification experiments with multiple files. It serves
as a canonical example of how audio analysis algorithms can be expressed
in the framework. This documentation refers to the latest refactored
version of bextract. The old-style @code{bextract} using the -e
command-line option to specify the feature extractor is still supported
but use of it discouraged.

Suppose that you want to build a real-time music/speech descriminator
based on a collection of music files named music.mf and a collection
of speech files named speech.mf.  These collections can either be
created manually or using the mkcollection utility. The following
commandline will extract means and variances of timbral features 
(time-domain Zero-Crossings, Spectral Centroid, Rolloff, Flux and 
Mel-Frequency Cepstral Coefficients (MFCC) over a texture window of 1 sec.  

@example 
bextract music.mf speech.mf -w ms.arff -p ms.mpl -cl GS
bextract ms.mf -w ms.arff -p ms.mpl 
bextract -mfcc classical.mf jazz.mf rock.mf -w genre.arff
@end example 

The first two commands are equivalent assuming that 
ms.mf is a labeled collection with the same files 
as music.mf and speech.mf. The third-command specifies 
that only the MFCC features should be extracted and is 
an example of classifying three classes. 

The results are stored in a ms.arff which is a text file storing the
feature values that can be used in the Weka machine learning environment
for experimentation with different classifiers.  After a header
describing the features (attribute in Weka terminology) it consists of
lines of comma separated feature values. Each line corresponds to a
feature vector. The attributes in the generated .arff file have long
descriptive names that show the process used to calculate the attribute.
In order to associate filenames and the subsequences of feature vectors
corresponding to them each subsequence corresponding to a file is
prefixed by the filename as a comment in the .arff file. It is a text
file that is straighforward to parse. Viewing it in a text editor will
make this clearer.

In addition to Weka, the native Marsyas kea tool @ref{kea} can be used
to perform evaluations (cross-validation, accuracies, confusion
matrices) similar to Weka although with more limited functionality.

At the same time that the features are extracted, a classifier (in the
example above a simple Naive Bayes classifier (or Gaussian)) is trained
and when feature extraction is completed the whole network of feature
extraction and classification is stored and can be used for real-time
audio classification directly as a Marsyas plugin stored in ms.mpl.

The resulting plugin makes a classification decision every 20ms but
aggregates the results by majority voting (using the Confidence
MarSystem) to display time-stamped output approximately every 1
second. The whole network is stored in ms.mpl which is loaded into
sfplugin and file_to_be_classified is played and classified at the same
time. The screen output shows the classification results and
confidence. The second command shows that the live run-time
classification can be integrated with @code{bextract}. In both cases 
collections can be used instead of single files. 

@example 
sfplugin -p ms.mpl music_file_to_be_classifed.wav
sfplugin -p ms.mpl speech_file_to_be_classifed.wav
bextract -e ms.mf -tc file_to_classified.wav 
bextract -e ms.mf -tc collection_to_classified.wav 
@end example 

Using the command-line option -sv turns on single vector 
feature extraction where one feature vector is extracted per file. 
The single-vector feature representation is useful for 
many Music Information Retrieval tasks (MIR) such 
as genre classification, similarity retrieval, and visualization 
of music collections. The following command can 
be used to generate a weka file for genre classification with one 
vector per file. 

@example 
./bextract -sv cl.mf ja.mf ro.mf -w genres.arff -p genres.mpl
@end example 

The resulting genres.arff file has only one feature vector line 
for each soundfile in the collections. In this case where 
no -cl command-line argument is specified a linear 
Support Vector Machine (SVM) classifier is used instead. 


Feature sets refer to collections of features that can 
be included in the feature extraction. It includes 
several individual feature sets proposed in the MIR 
and audio analysis literature as well as some common combinations 
of them. (for details and the most updated list of supported 
sets experienced users can consult the selectFeatureSet() function 
in bextract.cpp). The feature sets can be separated into three 
lagre groups depending what front-end is used: time-domain, 
spectral-domain, lpc-based. 

The following feature sets are supported (for definitions consult 
the MIR literature, check the corresponding code implementations 
and send us email with question for details you don't understand) : 

@table @samp
@item -timbral --TimbralFeatures 
      Time ZeroCrossings, Spectral  Centroid, Flux and Rolloff, 
and Mel-Frequency Cepstral Coefficients (MFCC). Equivalent to
-mfcc -zcrs -ctd -rlf -flx. This also the default extracted feature set. 

@item -spfe --SpectralFeatures
      Spectral Centroid, Flux and Rolloff. Equivalent to 
-zcrs -ctd -rlf -flx. 

@item -mfcc --MelFrequencyCepstralCoefficients
      Mel-Frequency Cepstral Coefficients.

@item -chroma --Chroma 
@item -ctd --SpectralCentroid
@item -rlf -- SpectralCentroid
@item -flx --SpectralFlux
@item -zcrs --ZeroCrossings
@item -sfm --SpectralFlatnessMeasure 
@item -scf --SpectralCrestFactor
@item -lsp --LineSpectralPair 
@item -lpcc --LinearPredictionCepstralCoefficients
@end table

By default stereo files are donwmixed to mono by summing the two
channels before extracting features.  However, bextract also supports
the extraction of feature based on stereo information. There are 
feature sets that can only be extracted from stereo files. In addition 
it is possible to use any of the feature sets described above 
and extract features for both left and right channels that are 
concatenated to form a feature vector. 

@table @samp 
@item -spsf --StereoPanningSpectrumFeatures 
@item -st --stereo
      Calculate whatever feature sets are activated for both left 
and right channels.  
@end table

For example the first command below calculates MFCC for both 
left and right channels.  The second command calculates 
the Stereo Panning Spectrum Features which require both 
channels and also the Spectral Centroid for both left 
and right. 

@example 
bextract -st -mfcc mymusic.mf -w mymusic.arff
bextract -spsf -st --SpectralCentroid -w mymusic.arff 
@end example 

The feature extraction can be configured in many ways 
(only some of which are possible through command-line 
options). The following options can be used to control 
various aspects of the feature extraction process
(most of the default values assume 22050 Hz sampling rate): 


@table @samp 
@item -c --collection
      the collection of files to be used
@item -s --start
      starting offset (in seconds) into each soundifle from which features will be extracted
@item -l --length
      length (in seconds) of each soundfile from which features will be extracted. A length of -1.0 indicates that the entire duration of the file should be used (the default behavior) 
@item -n --normalization
      apply normalization to audio signal
@item -fe --featExtract 
      only extract features without training the classifier 
@item -st --stereo
      use stereo feature extraction
@item -ds --downsample
      downsample factor (default 1)
@item -ws --winsamples
      size in samples of the analysis window (default 512) 
@item -hp --hopsamples 
      size in samples of the hop analysis size (default 512 - no overlap) 
@item -as --accSize 
      size in analysis frames of how many feature vectors are summarized 
when single vectors per file are calculated (default 1298 - approximately 30 seconds)
@item -m --memory 
      size in analysis frames of how many features vectors are summarized 
for each texture window (default 40 - approximately 1 second) 
@item -cl --classifier 
      classifier used for training and prediction (default GS - a simple Naive Bayes Classifier) 
@item -e --extractor 
      old-style specification of feature extraction maintained for backward compatibility (usage discouraged) 
@item -p --plugin 
      filename of generated Marsyas plugin (.mpl file) 
@item -w --wekafile 
      filename of generated .arff file (for Weka or kea)  
@item -tc --test 
      filename of collection or soundfile used for prediction 
after a model is trained (can be used to conduct MIREX style
experimetns) 
@item -pr --predict 
      filename of a collection or soundfile used for prediction 
after a model is trained 
@item -wd --workdir 
      Directory where all generated files will be written by 
defautl the current directory is used

@end table


@subsubheading TimeLines

bextract also supports a mode, called the Timeline mode that allows
labeling of different sections of an audio recording with different
labels. For example, you might have a number of audio files of Orca
recordings with sections of voiceover, background noise, and orca calls.
You could train a classifier to recognize each of these types of signal.
Instead of a label associated with each file in the collection 
there is an associate Marsyas timeline file (the format is described
below). To run bextract in Timeline mode, there are two steps:
training and classifier:

@example
bextract -t songs.mf -p out.mpl -pm

Where:

-t songs.mf - A collection file with a song name and its
corresponding .mtl (Marsyas Timeline) file on each line

-p out.mpl  - The Marsyas Plugin to be generated

-pm         - Mute the output plugin
@end example


and predicting labels for a new audio recording

@example
   sfplugin -p out.mpl songmono.wav

Where:

-p out.mpl   - The plugin output by bextract in step #1
@end example


The songs.mf file is Marsyas collection file with the path to song
(usually .wav) files and their corresponding Marsyas Timeline
(.mtl) files on each lines.  Here is an example song.mf file:

@example
  /path/to/song1.wav \t /path/to/song1.mtl
  /path/to/song2.wav \t /path/to/song2.mtl
  /path/to/song3.wav \t /path/to/song3.mtl
@end example

Please note that the separator character \t must be an actual tab, it
cannot be any other kind of whitespace.

The .mtl format has three header lines, followed by blocks of 4
lines for each annotated section.  The format is:

@example
  HEADER:
  -------

  number of regions
  line size (=1)
  total size (samples)

  FOR EACH SAMPLE:
  ----------------
  start (samples)
  classId (mrs_natural)
  end (samples)
  name (mrs_string)
@end example

  For example:
@verbatim
  3
  1
  2758127
  0
  0
  800000
  voiceover
  800001
  1
  1277761
  orca
  1277762
  2
  2758127
  background
@end verbatim

Because the .mtl file is kind of obtuse, we have written a small
Ruby program to convert Audacity label files to .mtl format.  This
script can be found at marsyas/scripts/generate-mtl.rb.  The
script is currently hardcoded to recognize the chord changes from
songs from the annotated Beatles archive, but you can easily
change this by modifying the "chords_array" variable.


@node ibt,  , bextract, Feature Extraction
@subsection @code{ibt}
@cindex ibt
 
@code{ibt} @cindex ibt 
-- standing for INESC-Porto Beat Tracker --
is a tempo induction and beat tracking system based on a competing 
multi-agent system that considers parallel hypotheses regarding tempo and beats.
The algorithm works either in real-time, for on-line systems, 
and off-line, for MIR-based applications.@*
Benchmarks on causal and noncausal versions reveal competitive results, 
under alternative conditions.@*
More technical information can be found in the paper 
``IBT: A Real-Time Tempo and Beat Tracking System'' 
published in the International Conference on Music Information Retrieval (ISMIR) 2010.
Online at: @uref{http://www.inescporto.pt/~fgouyon/docs/OliveiraGouyonMartinsReis_ISMIR2010.pdf}.@*@*
Examples of usage:

@example 
1. ibt foo.wav
2. ibt -a foo.wav
3. ibt -f foo.wav
4. ibt -mic
5. ibt -a -mic
6. ibt -f -mic
7. ibt -nc foo.wav
@end example 

@strong{1.} This causally processes foo.wav retrieving the processed beat times, in foo.txt,
and the respective median tempo, in foo_medianTempo.txt.@*
@b{NOTE:} By default an initial 5secs is used for the phase+tempo induction stage,
where the system is setup.@*
@strong{2.} Identical to @strong{1.} but playing audio with "clicks" on beats.@*
@strong{3.} Identical to @strong{1.} but saving a file with the audio + "clicks" on processed beats.@*
@strong{4.} This captures audio from microphone inpute and processes beats in real-time.@*
The processed beat times and tempo are saved, respectively, in mic.txt and mic_medianTempo.txt.@*
@b{NOTE:} Although IBT makes an effort to be noise robust it is still quite prone to it, so
a noise-clean environment is adviced for using the mode of operation.@*
@strong{5.} Identical to @strong{4}. but playing "clicks" on processed beats.@*
@strong{6.} Identical to @strong{4}. but saving a file with the captured audio + "clicks" on processed beats.@*
@strong{7.} This processes foo.wav non-causally (off-line mode), retrieving the the processed 
beat times, in foo.txt, and the respective median tempo, in foo_medianTempo.txt.@*
@b{NOTE:} this mode of operation working off-line provides better performance than 1., ideal
to MIR-based applications.

@table @samp 
@item -mic --microphone_input
      input sound via microphone interface.
@item -nc --non-causal
     for running in non-causal mode.
@item -t --induction_time
      time (in secs) dispended in the initial induction stage.
@item -o --output
	  output files (predicted beat times, mean/median tempo):@*
"beats", "medianTempo", "meanTempo", "beats+medianTempo", "beats+meanTempo", "beats+meanTempo+medianTempo", "none".
@item -b --backtrace
      after induction backtrace the analysis to the beginning (for causal [default] mode). 	  
@item -di --dumb_induction
      for ignoring period induction substituting it by manual defined values.
@item -a --audio
      play the original audio mixed with the synthesized beats.
@item -f --audiofile   
      output the original audio mixed with the synthesized beats (as fileName_beats.*).
@item -s --score_function
      heuristics which conducts the beat tracking: "regular" [default], "correlation", "squareCorr".
@item -m --metrical_time
      initial time (in secs) which allows tracking metrical changes (0 not allowing at all; -1 for the whole music [default]).
@item -l --log_file
      generate log file of the analysis.
@item -2b --givefirst2beats    
      replace induction stage with ground-truth 
(two first beats from beatTimes file - .txt or .beats - from the directory or file given as argument).
@item -2bs --givefirst2beats_startpoint
      equal to givefirst2beats mode but starting tracking at the first given beat time.
@item -1b --givefirst1beat
      replace initial phase by the given ground-truth first beat 
(from beatTimes file - .txt or .beats - from the directory or file given as argument).
@item -1bs --givefirst1beat_startpoint
      equal to givefirst1beat mode but start tracking at the given phase.
@item -pgt --giveinitperiod
      replace initial period given by the ibi of the ground-truth two first beats 
(from beatTimes file - .txt or .beats - from the directory or file given as argument).
@item -pgt_mr --giveinitperiod+metricalrel
      equal to giveinitperiod but complementing it with metrically related tempi (2x, 1/2x, 3x, 1/3x).
@item -pgt_nr --giveinitperiod+nonrel
      equal to giveinitperiod but complementing it with non-related tempi.
	  
@end table 



@node Synthesis, Machine Learning, Feature Extraction, Available tools
@section Synthesis

@menu
* phasevocoder::                
@end menu

@node phasevocoder,  , Synthesis, Synthesis
@subsection @code{phasevocoder}
@cindex phasevocoder

phasevocoder is probably the most powerful and canonical example of
sound synthesis provided currently by Marsyas. It is based on the
phasevocoder implementation described by F.R.Moore in his book
@qq{Elements of Computer Music}. It is broken into individual
MarSystems in a modular way and can be used for sound-file and real-time
input pitch-shifting and/or time-scaling. Several variations of the
algorithm proposed in the literature have been implemented and 
can be configured through several command-line options. Familiarity 
with phasevocoder terminology will help understanding their effect on
the transformed sound file. Some representative examples are: 
 
@example
phasevocoder foo.wav -f foo_identity.wav 
phasevocoder foo.wav -f foo_stretched.wav -n 2048 -w 2048 -d 256 -i 512 
phasevocoder foo.wav -ob -cm sorted -s 10 -p 1.5 -f foo_pitch_shifted.wav  
phasevocoder foo.wav -f foo_stretched.wav -n 4096 -w 4096 -d 768 -i 1024
-cm full -ucm identity_phaselock
phasevocoder foo.wav -f foo_stretched.wav -n 4096 -w 4096 -d 768 -i 1024
-cm analysis_scaled_phaselock -ucm scaled_phaselock
@end example

In the first example the input file foo.wav is passed through the classic
phasevocoder (overlap-add, FFT-frontend and FFT-backend) without any 
time or pitch modifications. The second example show how time stretching 
can be achieved by making the analysis hop size (-d) and the synthesis 
hop size (-i) different. The -n option specified the FFT size 
and the -w option specifies the window size. In the third example 
a bank of sinusoidal oscillators (-ob) is used instead of the
FFT-backend and the input is pitch shifted by 1.5. The fourth example 
uses identity phaselocking (-ucm) and the fifth example uses scaled
phaselocking (-cm and -ucm) as described by Laroche and Dobson. 

@table @samp 
@item -n --fftsize
      size of the fft
@item -w --winsize
      size of the window 
@item -v --voices
      number of voices
@item -g --gain
      linear volume gain
@item -b --bufferSize
      audio buffer size
@item -m --midi
      midi input port number
@item -e --epochHeterophonics
      heterophonics epoch
@item -d --decimation 
      analysis hop size (decimation)
@item -i --interpolation
      synthesis hop size (interpolation)
@item -p --pitchshift 
      pitch shift factor (for example 2.0 is an octave) 
@item -ob --oscbank 
      use bank of oscillators back-end 
@item -s --sinusoids 
      number of sinusoids to use if convert mode is sorted 
@item -cm --convertmode 
      analysis front-end mode: full: use all FFT bins, sorted: sort FFT
      bins by magnitude and only use s sinusoids,
      analysis_scaled_phaselock: compute extra analysis info for scaled
      phaselocking
@item -ucm --unconvertmode 
      synthesis back-end mode: classic: propagate phases for all bins,
      loose_phaselock: described by Puckette, identity_phaselock: pick
      peaks, propagate phases for peaks and lock regions of influence
      around them, scaled_phaselock: refinement that takes into account 
      information from the previous frame 
@item -on --onsets filename_with_onsets
      takes as input a simple text file with locations of onsets that 
      are used to re-initialize phases and not time stretch transient
      frames that contain the onsets. 
@end table







@node Machine Learning, Auditory Scene Analysis, Synthesis, Available tools
@section Machine Learning

@menu
* kea::                         
@end menu

@node kea,  , Machine Learning, Machine Learning
@subsection @code{kea}
@cindex kea

The previous version of Marsyas 0.1 contained machine learning 
functionality but until 2007 the new version 0.2 mostly relied on 
Weka for machine learning experiments. Although this  
situation was satisfactory for writing papers it was not possible 
to create real-time networks integrating machine learning. 
Therefore an effort was made to establish programming 
conventions for how machine learning MarSystems should be 
implemented. Last but not least we have always wanted to 
have as much functionality related to audio processing 
systems implemented natively in Marsyas. 

@code{kea} is one of the outcomes of this effort. Kea (a rare bird 
from New Zealand) is the Marsyas counterpart of Weka 
and provides similar capabilities with the command-line 
interface to Weka although much more limited (at least 
for now). 

Any weka .arff file can be used as input to kea 
although ususally the input is the extracted .arff 
files from @code{bextract}. The following command-line options 
are supported. 

@table @samp 
@item -m --mode
      specifies the mode of operation (train, distance_matrix, pca). The 
default mode is train. 
@item -cl --classifier
      the type of classifier to use if mode is train 
 Available classifiers are GS, ZEROR, SVM 
@item -w --wekefile 
      the name of the weka file 
@item -id --inputdir
      input directory
@item -od --outputdir
      output directory
@item -dm --distance_matrix 
      filename for the distance matrix output if mode is distance_matrix 
@end table

The main mode (train) basically performs 10-fold non-stratified 
cross-validation to evaluate the classification performance 
of the specified classifier on the provided .arff file. In addition 
to classification accuracy It outputs several other summary 
measures of the classifier's performance as well as the confusion 
matrix. The format of the output is similar to Weka. 

The mode distance_matrix is used to compute a NxN similarity matrix
based on the input .arff file containing N feature vector instances. The
output format is the one used for MIREX 2007 music similarity task. This
functionality relies on specific naming conventions related to the
Marsyas MIREX2007 submission. By default the output goes to dm.txt but
can be specified by the -dm command-line option. The following 
examples show different ways @code{kea} can be used. 

The pca mode reduces the input feature vectors by projecting 
them to the first 3 principal components using Principal 
Component Analysis (PCA). Each component is normalized 
to lie in the range [0-512]. The resulting transformed 
features are simply written to stdout. 

@example
kea -w iris.arff 
kea -m train -w iris.arff -cl SVM 
kea -m distance_matrix -dm dmatrix.txt -w iris.arff
kea -m pca -w iris.arff
@end example



@node Auditory Scene Analysis, Marsystem Interaction, Machine Learning, Available tools
@section Auditory Scene Analysis


@menu
* peakSynth::                   
* peakClustering::              
@end menu

@node peakSynth, peakClustering, Auditory Scene Analysis, Auditory Scene Analysis
@subsection @code{peakSynth}

@node peakClustering,  , peakSynth, Auditory Scene Analysis
@subsection @code{peakClustering}
@cindex peakClustering 

peakClustering performs sinusoidal analysis 
@cindex sinusoidal analysis 
followed by a spectral
clustering 
@cindex spectral clustering 
for grouping spectral peaks that operates across time and 
frequency over ``texture'' windows. It can be used for predominant 
melody separation on polyphonic audio signals. More technical 
information can be found in the paper ``Normalized Cuts for Predominant 
Melodic Source Separation'' published in the IEEE Transactions on Audio,
Speech and Language processing. Examples of usage: 

@example 
peakClustering foo.wav 
@end example 

This will result in a file named fooSep.wav that contains the separated 
predominant melody source such as a singing voice in a rock song or a
saxophone line in a jazz tune. 


@table @samp 
@item -n --fftsize         
      size of fft 
@item -w --winsize
      size of window 
@item -s --sinusoids       
      number of sinusoids per frame
@item -b --buffersize      
      audio buffer size
@item -o --outputdirectoryname   
     output directory path
@item -N --noisename 
      name of degrading audio file 
@item -p --panning 
      panning informations <foreground level (0..1)>-<foreground pan (-1..1)>-<background level>-<background pan> 
@item -t --typeSimilarity 
      similarity information a (amplitude) f (frequency) h (harmonicity)  
@item -q -quitAnalyse 
   quit processing after specified number f seconds
@item -T --textureSize
       number of frames in a texture window
@item -c -clustering 
    number of clusters in a texture window
@item -v --voices
    number of voices
@item -F --clusterFiltering
    cluster filtering
@item -A --attributes
    set attributes
@item -g --ground
    set ground
@item -SC --clusterSynthetize
    cluster synthetize
@item -P --peakStore
    set peak store
@item -k -keep 
      keep the specified number of clusters in the texture window 
@item -S --synthetise 
   synthetize using an oscillator bank (0), an IFFT mono (1), or an IFFT stereo (2)
@item -r --residual 
      output the residual sound (if the synthesis stage is selected)
@item -i --intervalFrequency 
   <minFrequency>_<maxFrequency> select peaks in this interval (default 250-2500 Hz)
@item -f --fileInfo 
      provide clustering parameters in the output name (s20t10i250_2500c2k1uTabfbho means 20 sines per frames in the 250_2500 Hz frequency Interval, 1 cluster selected among 2 in one texture window of 10 frames, no precise parameter estimation and using a combination of similarities abfbho)
@item -npp --noPeakPicking 
      do not perform peak picking in the spectrum
@item -u --unprecise 
   do not perform precise estimation of sinusoidal parameters
@item -if --ignoreFrequency
       ignore frequency similarity between peaks
@item -ia --ignoreAmplitude 
    ignore amplitude similarity between peaks
@item -ih --ignoreHWPS
       ignore harmonicity (HWPS) similarity between peaks
@item -ip --ignorePan
      ignore panning similarity between peaks
@item -uo --useOnsets
       use onset detector for dynamically adjusting the length of texture windows


@end table 







@node Marsystem Interaction, All of the above, Auditory Scene Analysis, Available tools
@section Marsystem Interaction

@menu
* sfplugin::                    
* msl::                         
@end menu

@node sfplugin, msl, Marsystem Interaction, Marsystem Interaction
@subsection @code{sfplugin}
@cindex sfplugin

sfplugin is the universal executable. Any network of Marsystems 
stored as a plugin can be loaded at run-time and sound can flow 
through the network. The following example with appropriate plugins 
will peform playback of foo.wav and playback with real time music
speech classification of foo.wav. 

@example 
sfplugin -p plugins/playback.mpl foo.wav
sfplugin -p musp_classify.mpl foo.wav
@end example 


Writing a basic sfplugin plugin

The sfplugin application expects that certain controls are available 
in any network it tries to handle. Therefore, one of the simplest 
demonstration plugins one can write is a plugin containing 
a SoundFileSource and an AudioSink, demonstrated below. 
As the sfplugin does not know were the sources and sinks are in the network
it is necessary to link the composite's controls with appropriate controls 
in the network. 

@example
// create the network that will become the plugin
MarSystem* sys = mng.create( "Series", "head" );

// create the two required MarSystems
sys->addMarSystem( mng.create( "SoundFileSource", "src" ) );
sys->addMarSystem( mng.create( "AudioSink", "dest" ) );

// while we don't actually want to play a file now, supply a valid
// filler name to keep the program happy; sfplugin will update it later
sys->updctrl( "SoundFileSource/src/mrs_string/filename",
	      "../../SuperGroovyLateralGeniculateNucleus.au" );

// since we're not playing the song now, set initAudio to false;
// sfplugin will update this to true when the network is executed there
sys->updctrl( "AudioSink/dest/mrs_bool/initAudio", false );

// set those pesky control links!
sys->linkctrl( "mrs_string/filename", "SoundFileSource/src/mrs_string/filename" );
sys->linkctrl( "mrs_bool/initAudio" , "AudioSink/dest/mrs_bool/initAudio" );
sys->linkctrl( "mrs_natural/pos"    , "SoundFileSource/src/mrs_natural/pos" );
sys->linkctrl( "mrs_bool/hasData"  , "SoundFileSource/src/mrs_bool/hasData" );

// finally, write the network to a file; the plugin can be run as
// follows: sfplugin -p some_plugin.mpl ../../SamsSavorySuperiorColliculus.au
ofstream ofs( "some_plugin.mpl" );
ofs << (*sys) << endl;
@end example



@node msl,  , sfplugin, Marsystem Interaction
@subsection @code{msl}
@cindex msl

One of the most useful and powerful characteristics of Marsyas 
is the ability to create and combine MarSystems at run time. 
msl (marsyas scripting language) is a simple interpreter 
that can be used to create dataflow networks, adjust controls, 
and run sound through the network. It's used as a backend for 
user interfaces therefore it has limited (or more accurately
non-existent) editing functionality. The current syntax 
is being revised so currently it's more a proof-of-concept. 
Msl has been inactive for a while as the SWIG bindings to 
scripting language seem to be the way to go. 

Here is an example of creating a simple network in msl and 
playing a sound file: 

@example 
msl 
[ msl ] create Series playbacknet
[ msl ] create SoundFileSource src
[ msl ] create Gain g
[ msl ] create AudioSink dest
[ msl ] add src  > playbacknet
[ msl ] add g    > playbacknet
[ msl ] add dest > playbacknet
[ msl ] updctrl playbacknet SoundFileSource/src/string/filename technomusic.au
[ msl ] run playbacknet
@end example 

The important thing to notice is that both the creation of MarSystems 
and their assembly into networks can be done at run-time without 
having to recompile any code. It also possible to use the GNU 
readline utility to provide more friendly user editing and 
command completion in Msl. 


@c keeping this in case Steven wants to resurrect anything from
@c the coffee tests.
@ignore
@n ode Tests
@s ection Tests

Marsyas contains a suite of tests which attempt@footnote{Somewhat
unsuccessfully, unfortunately.} to make sure that new functionality (or
bug fixes) do not cause bugs in existing (working) code.  The tests do
not require the marsyas executables to be installed (by default they
execute @file{bin/release/PROGNAME} ), but marsyas must be
compiled.

@warning{The tests also require Python to be installed.  Python is
installed by default on Linux and MacOS X machines; Windows users
may install it from @uref{http://python.org}.}

Developers interested in the inner workings of these tests may find the
information in @develref{Regression tests}.

@menu
* Sanity tests::                
* Coffee tests::                
* midiTest::                    
@end menu




@n ode Coffee tests
@s ubsection Coffee tests

These are a longer set of tests; they may take up to 5 minutes to
run.  They also require files that are not part of the Marsyas svn
or tarballs.  They are available on the web, see @ref{Datasets}.
The location of these files must be passed to the script, for
example

@example
scripts/commit_tests.py ../../marsyas-coffee/
@end example

Results will be printed on the command line, with more information in
the file @file{tests/results.log}.

These tests are a useful excuse for taking a coffee break.
@end ignore


@node All of the above,  , Marsystem Interaction, Available tools
@section All of the above

@menu
* mudbox::                      
@end menu

@node mudbox,  , All of the above, All of the above
@subsection @code{mudbox}
@cindex mudbox 

In computer science the term sandbox is frequently used to refer a
technique where a piece of software is isolated from the surrounding
operating system environment to reduce security risks. Unlike these
clean sandboxes in Marsyas the mudbox is a playground for
experimentation and messing around. This is all the more appropriate
given that several of the main Marsyas developers have lived in British
Columbia where sandbox turn into mudboxes most of the year. It was
motivated by the frequent need to experiment with various MarSystems
under construction without having to create a new application for each
case and potentially have to modify the build system. Typically code for
a MarSystem gets tested in mudbox, then is gradually expanded to a tool
and eventually becomes it's own application or gets integrated to one of
the existing ones. More information can be found @develref{Playing in
the mudbox}. The examples in mudbox are short and can provide quick
templates for various types of tasks. The specific example to be
executed is specified by the --toy-with, -t command-line argument:

@example 
mudbox -t onsets foo.wav 
mudbox --toy-with vibrato foo.wav 
mudbox -h 
@end example 

The first command will generate a stero file foo.wav_onsets.wav 
with one channel containing the detected onsets. The second 
command will apply a vibrato type of effect using a delay line 
to foo.wav. The third command will display many (but not necessarily) 
all of the available ``toys'' in mudbox. In general, mudbox is supposed
to be experimental so don't expect careful error checking or proper 
output messages. In most cases you will need to read the corresponding 
source code in mudbox.cpp to figure out what's happening. This is a
feature of mudbox not a bug :-). 

